\chapter*{Abstract}

In quantum computing, where algorithms
exist that can solve computational problems
more efficiently than any known classical
algorithms, the elimination of errors
that result from external disturbances
or from imperfect gates has become the
``holy grail,'' and a worldwide quest for
a large scale fault-tolerant and computationally
superior quantum computer is currently
taking place. Optimists rely on the premise
that, under a certain threshold of errors,
an arbitrary long fault-tolerant quantum
computation can be achieved with only
moderate (i.e., at most polynomial) overhead in computational cost.
Pessimists, on the other hand, object
that there are in principle (as opposed
to merely technological) reasons why such
machines are still inexistent, and that
no matter what gadgets are used, large
scale quantum computers will never be
computationally superior to classical
ones. Lacking a complete empirical characterization
of quantum noise, the debate on the physical
possibility of such machines invites philosophical
scrutiny. Making this debate more precise
by suggesting a novel statistical mechanical
perspective thereof is the goal of this project.
